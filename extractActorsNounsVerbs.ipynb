{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP16eF6vJA4VteA+J0thqDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-King-00/Gp-AutomationOfBaTasks/blob/tony/extractActorsNounsVerbs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0RNbPb69th5"
      },
      "outputs": [],
      "source": [
        "! pip3 install PyGithub\n",
        "from github import Github\n",
        "\n",
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/T-King-00/Gp-AutomationOfBaTasks/tony/university.txt')\n",
        "file = response.text\n",
        "\n",
        "\n",
        "! pip install -U spacy\n",
        "from spacy.language import Language\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-1]):\n",
        "    \n",
        "        if token.text == \".\"  :\n",
        "          doc[i + 1].is_sent_start = True\n",
        "        elif  token.text == \"As\":\n",
        "          doc[i].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgmhpl6V9v2s",
        "outputId": "cd1d0321-d65d-4fe5-bc8a-90317986bab7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.custom_sentencizer(doc)>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHUHT4Q3HKWB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(file)\n",
        "sentences=[]\n",
        "for sent in doc.sents:\n",
        "  sentences.append(sent.text)"
      ],
      "metadata": {
        "id": "dIeadxlX-ApH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor():\n",
        "\n",
        "  def __init__(self,actorName):\n",
        "   self.name=actorName\n",
        "   self.usecases=[]\n",
        "  def addUseCase(self,useCase):\n",
        "    self.usecases.append(useCase)\n",
        "\n",
        "###############################"
      ],
      "metadata": {
        "id": "B-x1Y5K-_5-G"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actors=[]\n",
        "for sentenceObj in sentences:\n",
        "  objNlp=nlp(sentenceObj)\n",
        "  ### this part get compound nouns (actors))\n",
        "  for chunk in objNlp.noun_chunks:\n",
        "    actorObj=Actor(chunk.text)\n",
        "    if any(obj.name == actorObj.name for obj in actors):\n",
        "      break\n",
        "    else:\n",
        "      actors.append(actorObj)\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "print(\"Actors are #################\")\n",
        "print(len(actors))\n",
        "for act in actors:\n",
        "  print(act.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-jrTfVz_wBN",
        "outputId": "1c3eb772-a93d-4c7c-f704-b09ab530915a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actors are #################\n",
            "9\n",
            "a participant\n",
            "an instructor\n",
            "a site visitor\n",
            "a search engine\n",
            "a site owner\n",
            "a trainer\n",
            "a buyer\n",
            "a user\n",
            "a company\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### nouns \n",
        "nouns=[]\n",
        "for sentenceObj in sentences:\n",
        "  objNlp=nlp(sentenceObj)\n",
        "  ### this part get compound nouns (actors))\n",
        "  for i,chunk in enumerate(objNlp.noun_chunks):\n",
        "    if i==0:\n",
        "      continue;\n",
        "    if chunk.text not in nouns:\n",
        "      nouns.append(chunk.text)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "for x in nouns:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTrPGDRFFXav",
        "outputId": "e417ccfb-e19e-47fb-8943-4bf6f4297bc4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "the site\n",
            "the midst\n",
            "a quiz\n",
            "video\n",
            "my progress\n",
            "a forum\n",
            "a course\n",
            "questions\n",
            "topics\n",
            "the instructor\n",
            "other participants\n",
            "announcements\n",
            "a project “home page\n",
            "a Scheduled Course\n",
            "participants\n",
            "thinks\n",
            "chapter\n",
            "an instructor\n",
            "date\n",
            "news\n",
            "quizzes\n",
            "their learning\n",
            "part\n",
            "feedback\n",
            "material\n",
            "the previous video(s\n",
            "it\n",
            "quiz responses\n",
            "the database\n",
            "people\n",
            "various questions\n",
            "that\n",
            "which quizzes\n",
            "the course outline\n",
            "a comment\n",
            "the world\n",
            "what\n",
            "each quiz question\n",
            "the question\n",
            "an answer\n",
            "a different order\n",
            "an FAQ\n",
            "all my questions\n",
            "answers\n",
            "text\n",
            "a site map\n",
            "all pages\n",
            "all feedback\n",
            "historical or aggregate data\n",
            "my courses\n",
            "my account\n",
            "my current information\n",
            "a list\n",
            "all courses\n",
            "the PDUs\n",
            "each\n",
            "me\n",
            "access\n",
            "that historical information\n",
            "someone\n",
            "videos\n",
            "those videos\n",
            "a video course\n",
            "those quizzes\n",
            "they\n",
            "the elements\n",
            "course\n",
            "them\n",
            "an amazing course\n",
            "a discussion forum\n",
            "some\n",
            "not all courses\n",
            "the schedule\n",
            "prospective participants\n",
            "what course\n",
            "elements\n",
            "the publication date\n",
            "course elements\n",
            "a scheduled course\n",
            "items\n",
            "subtitles\n",
            "any video\n",
            "the thumbnail image\n",
            "each video\n",
            "the image\n",
            "all videos\n",
            "a Front Row Agile logo\n",
            "the videos\n",
            "the Front Row Agile site\n",
            "the prices\n",
            "my course\n",
            "filthy lucre\n",
            "quantity thresholds\n",
            "discounts\n",
            "lower prices\n",
            "larger orders\n",
            "10%\n",
            "discount codes\n",
            "pricing\n",
            "courses\n",
            "my shopping cart\n",
            "watch\n",
            "one chapter\n",
            "just a part\n",
            "the courses\n",
            "my cart\n",
            "a credit card\n",
            "PayPal\n",
            "a receipt\n",
            "any course\n",
            "my participation\n",
            "various times\n",
            "various social media sites\n",
            "others\n",
            "the license terms\n",
            "a monthly or annual subscription\n",
            "multiple licenses\n",
            "course registrations\n",
            "a group\n",
            "a set\n",
            "registrations\n",
            "which\n",
            "my company\n",
            "a site license\n",
            "everyone\n",
            "the course\n",
            "each course element\n",
            "a video\n",
            "parts\n",
            "some videos\n",
            "the full course\n",
            "a mode\n",
            "one\n",
            "the previous one finishes\n",
            "my keyboard\n",
            "mouse\n",
            "screen\n",
            "a page\n",
            "credits\n",
            "the credit\n",
            "a certificate\n",
            "completion\n",
            "proof\n",
            "a badge\n",
            "that badge\n",
            "my own website\n",
            "a sample video\n",
            "other comments\n",
            "a description\n",
            "a catalog\n",
            "a bio\n",
            "myself\n",
            "a detailed page\n",
            "each course\n",
            "other related courses\n",
            "other courses\n",
            "an evaluation form\n",
            "FrontRowAgile.com\n",
            "their classes\n",
            "their devilishly good-looking trainers\n",
            "an email\n",
            "every course evaluation\n",
            "a participant\n",
            "real-time feedback\n",
            "a privacy policy\n",
            "all the training courses\n",
            "all the great stuff\n",
            "a CSV file\n",
            "-depth\n",
            "a newsletter\n",
            "new courses\n",
            "other information\n",
            "“Featured Products\n",
            "the home page\n",
            "the products\n",
            "all visitors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### verbs \n",
        "verbs=[]\n",
        "for sentenceObj in sentences:\n",
        "  objNlp=nlp(sentenceObj)\n",
        "  ### this part get verbs)\n",
        "  for i,tok in enumerate(objNlp):\n",
        "    if tok.pos_==\"VERB\" and tok.lemma_ not in verbs:\n",
        "      verb=tok.lemma_\n",
        "      verbs.append(verb)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "for x in verbs:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtCTStDI1tl",
        "outputId": "f9addda1-12ac-4093-e84f-6a07ba840115"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "want\n",
            "shut\n",
            "’m\n",
            "interrupt\n",
            "participate\n",
            "discuss\n",
            "post\n",
            "tell\n",
            "delay\n",
            "read\n",
            "keep\n",
            "create\n",
            "assess\n",
            "take\n",
            "get\n",
            "learn\n",
            "see\n",
            "pass\n",
            "finish\n",
            "store\n",
            "do\n",
            "fix\n",
            "word\n",
            "view\n",
            "know\n",
            "need\n",
            "re\n",
            "-\n",
            "leave\n",
            "think\n",
            "receive\n",
            "navigate\n",
            "submit\n",
            "press\n",
            "answer\n",
            "watch\n",
            "index\n",
            "log\n",
            "manage\n",
            "reflect\n",
            "earn\n",
            "have\n",
            "call\n",
            "upload\n",
            "include\n",
            "ensure\n",
            "sequence\n",
            "assemble\n",
            "indicate\n",
            "add\n",
            "present\n",
            "set\n",
            "become\n",
            "like\n",
            "select\n",
            "watermarke\n",
            "make\n",
            "offer\n",
            "buy\n",
            "give\n",
            "customise\n",
            "put\n",
            "purchase\n",
            "pay\n",
            "prove\n",
            "announce\n",
            "subscribe\n",
            "sign\n",
            "control\n",
            "understand\n",
            "mark\n",
            "complete\n",
            "decide\n",
            "begin\n",
            "touch\n",
            "show\n",
            "display\n",
            "respond\n",
            "browse\n",
            "pick\n",
            "maintain\n",
            "use\n",
            "list\n",
            "relate\n",
            "look\n",
            "opt\n",
            "’\n",
            "link\n",
            "share\n",
            "export\n",
            "perform\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### compund verbs  phrasalverbs\n",
        "matcher=Matcher(nlp.vocab)\n",
        "compoundVerbs=[]\n",
        "\n",
        "for sentenceVar in sentences: \n",
        "  v = sentenceVar.find(\"so that\")\n",
        "  sentenceVar=sentenceVar[0:v]\n",
        "  x=nlp(sentenceVar)\n",
        "\n",
        "  pattern0 = [[{\"LOWER\": \"want\"}, {\"LOWER\": \"to\" },{\"POS\":\"VERB\"},{\"POS\":\"DET\",\"OP\":\"*\"},{\"POS\":\"NOUN\"}]\n",
        "              ,[{\"LOWER\": \"make\"}, {\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern1=[{\"POS\":\"VERB\"},{\"POS\":\"NOUN\"}]\n",
        "  pattern2=[[{\"POS\":\"VERB\"},{\"POS\":\"DET\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"VERB\"},{\"LOWER\":\"the\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern3=[{\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]\n",
        "  matcher.add(\"verbPhrase\", [pattern1])\n",
        "  matcher.add(\"verbPhrase2\", pattern2)\n",
        "\n",
        "\n",
        "  matches = matcher(x)\n",
        "  for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
        "      span = x[start:end]  # The matched span\n",
        "      if span.text.find(\"want the\")>-1:\n",
        "        continue\n",
        "      compoundVerbs.append(span)\n",
        "      \n",
        "compoundVerbs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh256iwwK5TK",
        "outputId": "09faf049-98e0-43b9-9d1f-fca4f97a70cb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[post announcements,\n",
              " read announcements,\n",
              " create quizzes,\n",
              " take a quiz,\n",
              " pass a quiz,\n",
              " want quiz,\n",
              " see which quizzes,\n",
              " viewing the course,\n",
              " leave a comment,\n",
              " receive feedback,\n",
              " navigate a quiz,\n",
              " submitting an answer,\n",
              " read an FAQ,\n",
              " get answers,\n",
              " view a site,\n",
              " view all feedback,\n",
              " view a list,\n",
              " upload videos,\n",
              " create a quiz,\n",
              " sequence the elements,\n",
              " indicate the schedule,\n",
              " set the publication,\n",
              " scheduled course,\n",
              " add subtitles,\n",
              " set the thumbnail,\n",
              " want all videos,\n",
              " set the prices,\n",
              " set quantity,\n",
              " create discount,\n",
              " put courses,\n",
              " receive a receipt,\n",
              " view the license,\n",
              " manage a set,\n",
              " purchase a site,\n",
              " watch any video,\n",
              " want each course,\n",
              " view some videos,\n",
              " set a mode,\n",
              " shown a page,\n",
              " learning credits,\n",
              " complete the course,\n",
              " earn a certificate,\n",
              " finishing a course,\n",
              " earn a badge,\n",
              " completed a course,\n",
              " leave a comment,\n",
              " create a description,\n",
              " browse a catalog,\n",
              " maintain a bio,\n",
              " see a list,\n",
              " related courses,\n",
              " viewing a course,\n",
              " shown an evaluation,\n",
              " completing a course,\n",
              " receiving an email,\n",
              " read a privacy,\n",
              " have a page,\n",
              " export all feedback]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "class UserStory():\n",
        "  def __init__(self):\n",
        "   self.actors=[]\n",
        "   self.usecases=[]\n",
        " \n",
        "  def addActor(self,text):\n",
        "    if text not in self.actors :\n",
        "      self.actors.append(text)\n",
        "\n",
        "  def checkSimilarityB2UseCases(self,t1):\n",
        "    for x in self.usecases:\n",
        "      print(t1,\"---\",x)\n",
        "      v = re.search(t1,x)\n",
        "      if v :\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "  def addUseCase(self,text):\n",
        "    if text not in self.usecases and text!=\"want\":\n",
        "      v = text.find(\"want the\")\n",
        "      if v >-1:\n",
        "        return 0\n",
        "      if self.checkSimilarityB2UseCases(text):\n",
        "        self.usecases.append(text)\n",
        " \n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "NL9ivie2-DCf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}