{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-King-00/Gp-AutomationOfBaTasks/blob/tony/practicingNlp3-3-2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install PyGithub\n",
        "from github import Github\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpBTVs_RdIyy",
        "outputId": "54a0eb3f-a44b-4d9b-e9f7-52e3fb361e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-1.58.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt>=2.4.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting pynacl>=1.4.0\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.8/dist-packages (from PyGithub) (2.25.1)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from pynacl>=1.4.0->PyGithub) (1.15.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (1.26.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated->PyGithub) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, deprecated, pynacl, PyGithub\n",
            "Successfully installed PyGithub-1.58.0 deprecated-1.2.13 pyjwt-2.6.0 pynacl-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/T-King-00/Gp-AutomationOfBaTasks/tony/atmUserStories.txt')\n",
        "file = response.text"
      ],
      "metadata": {
        "id": "GD14FYmzMr_T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install -U spacy\n",
        "from spacy.language import Language\n",
        "import spacy"
      ],
      "metadata": {
        "id": "gSixyAQyCOVF",
        "outputId": "7fe44a31-f8e5-416a-d728-6340a1173c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class UserStory():\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "   self.actors=[]\n",
        "   self.usecases=[]\n",
        " \n",
        "  def addActor(self,text):\n",
        "    if text not in self.actors :\n",
        "      self.actors.append(text)\n",
        "\n",
        "  def checkSimilarityB2UseCases(self,t1):\n",
        "    for x in self.usecases:\n",
        "      print(t1,\"---\",x)\n",
        "      v = re.search(t1,x)\n",
        "      if v :\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "  def addUseCase(self,text):\n",
        "\n",
        "    if text not in self.usecases and text!=\"want\":\n",
        "      if self.checkSimilarityB2UseCases(text):\n",
        "        self.usecases.append(text)\n",
        " \n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1leYVrsN7pDg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-1]):\n",
        "    \n",
        "        if token.text == \".\"  :\n",
        "          doc[i + 1].is_sent_start = True\n",
        "        elif  token.text == \"As\":\n",
        "          doc[i].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxKoDEN8Twgk",
        "outputId": "24c730b9-1dc5-4dec-ae8f-7c9f0c05043b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.custom_sentencizer(doc)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###try 1\n",
        "doc = nlp(file)\n",
        "sentences=[]\n",
        "for sent in doc.sents:\n",
        "  sentences.append(sent.text)"
      ],
      "metadata": {
        "id": "T86Dq2m8Y6LT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parse the user story with Spacy\n",
        "doc5 = nlp(sentences[9])\n",
        "\n",
        "# Find the main verb of the sentence\n",
        "\n",
        "main_verb = [token for token in doc5 if token.pos_ == \"VERB\"][1]\n",
        "\n",
        "# Find the direct object of the main verb\n",
        "direct_object = [child for child in main_verb.children if child.dep_ == \"dobj\"][0]\n",
        "action = [token.text for token in doc5 if token.dep_ == \"xcomp\"]\n",
        "\n",
        "print(spacy.explain(\"xcomp\"))\n",
        "\n",
        "# Find the verb phrase that serves as the direct object\n",
        "vp_object = \"\"\n",
        "for token in direct_object.subtree:\n",
        "    vp_object += token.text + \" \"\n",
        "\n",
        "# Print the verb phrase object\n",
        "print(\"The verb phrase object in the user story is:\", main_verb ,vp_object.strip())\n",
        "print(\"action\",action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "7bMcj14Wlx3M",
        "outputId": "b128519b-b151-48f2-cc79-cb2796662de0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-f73703c64cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Parse the user story with Spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Find the main verb of the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a user story as a string\n",
        "user_story = \"As a Customer, I want to deposit check in my bank account through ATM so that I may save my time and perform transactions later.\"\n",
        "\n",
        "# Parse the user story with Spacy\n",
        "doc = nlp(user_story)\n",
        "\n",
        "# Extract the action, user, and goal\n",
        "action = [token.text for token in doc if token.dep_ == \"xcomp\"][0]\n",
        "user = [token.text for token in doc if token.dep_ == \"nsubj\"][0]\n",
        "goal = [token.text for token in doc if token.dep_ == \"mark\"] + [token.text for token in doc if token.dep_ == \"advcl\"]\n",
        "\n",
        "# Extract the object by finding the direct object of the action\n",
        "object = [token.text for token in doc if token.dep_ == \"dobj\" and token.head.text == action][0]\n",
        "\n",
        "# Print the extracted object\n",
        "print(\"The object in the user story is:\", object)\n"
      ],
      "metadata": {
        "id": "m2HWs7pxlS7m",
        "outputId": "8da8be0b-d7e3-4578-f357-b50557496f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The object in the user story is: check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model in spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the input sentence\n",
        "sentence = \"As a Ticket Seller I want to be able to sell tickets for a current festival to meet my sales goals.\"\n",
        "\n",
        "# Parse the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract the key components of the sentence\n",
        "subject = None\n",
        "verb = None\n",
        "object = None\n",
        "goal = None\n",
        "festival = None\n",
        "\n",
        "for token in doc:\n",
        "    # Find the subject of the sentence (a noun or pronoun)\n",
        "    if token.dep_ == 'nsubj' and token.pos_ in ['NOUN', 'PRON']:\n",
        "        subject = token.text\n",
        "    # Find the main verb of the sentence\n",
        "    elif token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
        "        verb = token.text\n",
        "    # Find the object of the sentence (a noun or noun phrase)\n",
        "    elif token.dep_ == 'dobj' and token.pos_ in ['NOUN', 'PROPN']:\n",
        "        object = token.text\n",
        "    # Find the goal of the Ticket Seller\n",
        "    elif token.text == 'meet' and token.head.text == 'to':\n",
        "        goal = token.head.head.text\n",
        "    # Find the festival for which the tickets are being sold\n",
        "    elif token.text == 'festival' and token.head.text == 'for':\n",
        "        festival = token.head.head.text\n",
        "\n",
        "# Print the extracted components\n",
        "print(f\"Subject: {subject}\")\n",
        "print(f\"Verb: {verb}\")\n",
        "print(f\"Object: {object}\")\n",
        "print(f\"Goal: {goal}\")\n",
        "print(f\"Festival: {festival}\")\n"
      ],
      "metadata": {
        "id": "B4Dt9QlmfA27",
        "outputId": "90958b4a-48f0-4929-9d01-cc0a776ffdb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: I\n",
            "Verb: want\n",
            "Object: goals\n",
            "Goal: None\n",
            "Festival: sell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_components(user_story):\n",
        "    # Parse the user story using spaCy\n",
        "    doc = nlp(user_story)\n",
        "\n",
        "    # Extract the key components of the sentence\n",
        "    subject = None\n",
        "    verb = None\n",
        "    object = None\n",
        "    goal = None\n",
        "    target = None\n",
        "\n",
        "    for token in doc:\n",
        "        # Find the subject of the sentence (a noun or pronoun)\n",
        "        if token.dep_ == 'nsubj' and token.pos_ in ['NOUN', 'PRON']:\n",
        "            subject = token.text\n",
        "        # Find the main verb of the sentence\n",
        "        elif token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
        "            verb = token.text\n",
        "        # Find the object of the sentence (a noun or noun phrase)\n",
        "        elif token.dep_ == 'dobj' and token.pos_ in ['NOUN', 'PROPN']:\n",
        "            object = token.text\n",
        "        # Find the goal of the user story\n",
        "        elif token.text in ['achieve', 'meet', 'fulfill'] and token.head.text == 'to':\n",
        "            goal = token.head.head.text\n",
        "        # Find the target of the user story\n",
        "        elif token.text in ['for', 'in'] and token.head.text in ['sell', 'buy', 'purchase']:\n",
        "            target = token.head.head.text\n",
        "\n",
        "    # Construct a dictionary with the extracted components\n",
        "    components = {\n",
        "        'subject': subject,\n",
        "        'verb': verb,\n",
        "        'object': object,\n",
        "        'goal': goal,\n",
        "        'target': target\n",
        "    }\n",
        "\n",
        "\n",
        "    return components\n",
        "\n",
        "print(sentences[3])\n",
        "comp=extract_components(sentences[3])\n",
        "print(comp )\n"
      ],
      "metadata": {
        "id": "tkhzw5ZKi_vg",
        "outputId": "8d94968f-b397-43f8-8de8-2f6b9f52aed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Customer ,I want to deposit check in my bank account through ATM so that I may save my time and perform transactions later.\r\n",
            "\n",
            "{'subject': 'I', 'verb': 'want', 'object': 'transactions', 'goal': None, 'target': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Parse the message with spaCy\n",
        "doc = nlp(\" As a Ticket Seller I want to be able to sell tickets for a current festival to meet my sales goals.\")\n",
        "print(doc)\n",
        "# Find the relevant nouns and adjectives in the message\n",
        "nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
        "adjectives = [token.text for token in doc if token.pos_ == 'ADJ']\n",
        "\n",
        "# Find the subject and object of the message\n",
        "subject = [token.text for token in doc if token.dep_ == 'nsubj']\n",
        "objects = [token.text for token in doc if token.dep_ == 'pobj']\n",
        "root = [token.text for token in doc if token.dep_ == 'root']\n",
        "\n",
        "# Find the preposition and its object\n",
        "preposition = [token.text for token in doc if token.dep_ == 'prep']\n",
        "preposition_object = [token.text for token in doc if token.dep_ == 'pobj']\n",
        "verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
        "\n",
        "# Print the extracted information\n",
        "print(\"Nouns:\", nouns)\n",
        "print(\"Adjectives:\", adjectives)\n",
        "print(\"Subject:\", subject)\n",
        "print(\"Object:\", objects)\n",
        "print(\"Preposition:\", preposition)\n",
        "print(\"Preposition Object:\", preposition_object)\n",
        "print(\"verbs :\", verbs)\n",
        "print(\"verbs :\", verbs)\n"
      ],
      "metadata": {
        "id": "4Q3Tfrw1cK4e",
        "outputId": "4cfc90de-4f05-4c28-ac63-68d62eb52379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " As a Ticket Seller I want to be able to sell tickets for a current festival to meet my sales goals.\n",
            "Nouns: ['Ticket', 'tickets', 'festival', 'sales', 'goals']\n",
            "Adjectives: ['able', 'current']\n",
            "Subject: ['I']\n",
            "Object: ['Seller', 'festival']\n",
            "Preposition: ['As', 'for']\n",
            "Preposition Object: ['Seller', 'festival']\n",
            "verbs : ['want', 'sell', 'meet']\n",
            "verbs : ['want', 'sell', 'meet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the sample sentence\n",
        "sentence = \"As a Ticket Seller I want to be able to sell tickets for a current festival to meet my sales goals.\"\n",
        "\n",
        "# Parse the sentence with spaCy\n",
        "doc = nlp(sentences[0])\n",
        "\n",
        "# Print the dependencies of each word in the sentence\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])\n"
      ],
      "metadata": {
        "id": "QJZ4GM1xdUu0",
        "outputId": "12cc3cf9-c36b-4c25-a416-459ac0870e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As prep want VERB [Customer]\n",
            "a det Customer NOUN []\n",
            "Customer pobj As ADP [a]\n",
            ", punct want VERB []\n",
            "I nsubj want VERB []\n",
            "want ROOT want VERB [As, ,, I, login, .]\n",
            "to aux login VERB []\n",
            "login xcomp want VERB [to, to, using, perform]\n",
            "to prep login VERB [account]\n",
            "my poss account NOUN []\n",
            "account pobj to ADP [my]\n",
            "using advcl login VERB [card]\n",
            "card dobj using VERB [and, code]\n",
            "and cc card NOUN []\n",
            "PIN compound code NOUN []\n",
            "code conj card NOUN [PIN]\n",
            "so mark perform VERB []\n",
            "that mark perform VERB []\n",
            "I nsubj perform VERB []\n",
            "can aux perform VERB []\n",
            "perform advcl login VERB [so, that, I, can, transactions]\n",
            "the det transactions NOUN []\n",
            "transactions dobj perform VERB [the]\n",
            ". punct want VERB []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc.ents #used for named entites like websites \n",
        "\"\"\"\n",
        "This doc property is used for the named entities in the document. If the entity recognizer has been applied, this property will return a tuple of named entity span objects.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SeWuQNyJ_ZIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This part gets the actors in all user stories through looping through each sentence and getting noun chunks .**"
      ],
      "metadata": {
        "id": "BTI3rC0x_avK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USERSTORIES=UserStory()\n",
        "\n",
        "for sentenceObj in sentences:\n",
        "  objNlp=nlp(sentenceObj)\n",
        "\n",
        "  ### this part get compound nouns (actors))\n",
        "  for chunk in objNlp.noun_chunks:\n",
        "    USERSTORIES.addActor(chunk.text)\n",
        "    break\n",
        "\n",
        "USERSTORIES.actors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2RPiazqz1THZ",
        "outputId": "707ebe70-2c46-4512-9f16-ec31114d7b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a Customer']"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "  for ent in objNlp:\n",
        "    print(ent.text,\" -> \",ent.pos_,\"   \",ent.tag_,\"    \",spacy.explain(ent.pos_)) \n",
        "    if ent.pos_==\"NOUN\" :\n",
        "      USERSTORY.addActor(ent.text)\n",
        "      break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RASJ3frs92lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This part to get use cases of one actor .**"
      ],
      "metadata": {
        "id": "UIOFteGV_pSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "gdFt5lnPR0OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher=Matcher(nlp.vocab)\n",
        "\n",
        "for sentenceVar in sentences:\n",
        "  usecases=[]\n",
        "  print(sentenceVar)\n",
        "  v = sentenceVar.find(\"so that\")\n",
        "  sentenceVar=sentenceVar[0:v]\n",
        "  print(sentenceVar)  \n",
        "  x=nlp(sentenceVar)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "  tokens=[token for token in x if not token.is_stop]\n",
        "  print(tokens)\n",
        "\n",
        "  pattern0 = [[{\"LOWER\": \"want\"}, {\"LOWER\": \"to\" },{\"POS\":\"VERB\"},{\"POS\":\"DET\",\"OP\":\"*\"},{\"POS\":\"NOUN\"}]\n",
        "              ,[{\"LOWER\": \"make\"}, {\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern1=[{\"POS\":\"VERB\"},{\"POS\":\"NOUN\"}]\n",
        "  pattern2=[[{\"POS\":\"VERB\"},{\"POS\":\"DET\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"VERB\"},{\"LOWER\":\"the\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern3=[{\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]\n",
        "  matcher.add(\"verbPhrase\", [pattern1])\n",
        "  matcher.add(\"verbPhrase2\", pattern2)\n",
        "\n",
        "\n",
        "  matches = matcher(x)\n",
        "  for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
        "      span = x[start:end]  # The matched span\n",
        "      print(match_id, string_id, start, end,\"text:\", span.text)\n",
        "  \n",
        "      USERSTORIES.addUseCase(span.text)\n",
        "\n",
        "  \n",
        "  for token in x:\n",
        "    if token.is_stop !=True:\n",
        "      \n",
        "      if token.pos_== \"VERB\" :\n",
        "        USERSTORIES.addUseCase(token.text)\n",
        "        print(token.text)\n",
        "  print(\"use cases are : \" ,USERSTORIES.usecases)\n",
        "\n",
        "  print(\"end of loopp#############################################\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhkUrl0O_mji",
        "outputId": "a50f9c99-fb9d-44d3-93db-bd5ec44b1f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Customer ,I want to login to my account using card and PIN code so that I can perform the transactions.\n",
            "As a Customer ,I want to login to my account using card and PIN code \n",
            "[Customer, ,, want, login, account, card, PIN, code]\n",
            "13289390234190212481 verbPhrase 11 13 text: using card\n",
            "want\n",
            "login --- using card\n",
            "login\n",
            "use cases are :  ['using card', 'login']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to  check the balance of my bank account so that I can perform transactions.\n",
            "As a Customer ,I want to  check the balance of my bank account \n",
            "[Customer, ,, want,  , check, balance, bank, account]\n",
            "7260299203751380718 verbPhrase2 8 11 text: check the balance\n",
            "check the balance --- using card\n",
            "check the balance --- login\n",
            "want\n",
            "check --- using card\n",
            "check --- login\n",
            "check --- check the balance\n",
            "check\n",
            "use cases are :  ['using card', 'login', 'check the balance']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to  deposit cash in my bank account through ATM so that I may save my time and perform transactions later.\n",
            "As a Customer ,I want to  deposit cash in my bank account through ATM \n",
            "[Customer, ,, want,  , deposit, cash, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 8 10 text: deposit cash\n",
            "deposit cash --- using card\n",
            "deposit cash --- login\n",
            "deposit cash --- check the balance\n",
            "want\n",
            "deposit --- using card\n",
            "deposit --- login\n",
            "deposit --- check the balance\n",
            "deposit --- deposit cash\n",
            "deposit\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to deposit check in my bank account through ATM so that I may save my time and perform transactions later.\n",
            "As a Customer ,I want to deposit check in my bank account through ATM \n",
            "[Customer, ,, want, deposit, check, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: deposit check\n",
            "deposit check --- using card\n",
            "deposit check --- login\n",
            "deposit check --- check the balance\n",
            "deposit check --- deposit cash\n",
            "want\n",
            "deposit --- using card\n",
            "deposit --- login\n",
            "deposit --- check the balance\n",
            "deposit --- deposit cash\n",
            "deposit\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check']\n",
            "end of loopp#############################################\n",
            "As a Customer I want to withdraw cash from my bank account through ATM so that I may save my time.\n",
            "As a Customer I want to withdraw cash from my bank account through ATM \n",
            "[Customer, want, withdraw, cash, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 6 8 text: withdraw cash\n",
            "withdraw cash --- using card\n",
            "withdraw cash --- login\n",
            "withdraw cash --- check the balance\n",
            "withdraw cash --- deposit cash\n",
            "withdraw cash --- deposit check\n",
            "want\n",
            "withdraw --- using card\n",
            "withdraw --- login\n",
            "withdraw --- check the balance\n",
            "withdraw --- deposit cash\n",
            "withdraw --- deposit check\n",
            "withdraw --- withdraw cash\n",
            "withdraw\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash']\n",
            "end of loopp#############################################\n",
            "As a Customer, I want to transfer money from my account to another bank account through ATM so that I may save my time.\n",
            "As a Customer, I want to transfer money from my account to another bank account through ATM \n",
            "[Customer, ,, want, transfer, money, account, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: transfer money\n",
            "transfer money --- using card\n",
            "transfer money --- login\n",
            "transfer money --- check the balance\n",
            "transfer money --- deposit cash\n",
            "transfer money --- deposit check\n",
            "transfer money --- withdraw cash\n",
            "want\n",
            "transfer --- using card\n",
            "transfer --- login\n",
            "transfer --- check the balance\n",
            "transfer --- deposit cash\n",
            "transfer --- deposit check\n",
            "transfer --- withdraw cash\n",
            "transfer --- transfer money\n",
            "transfer\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "\n",
            "\n",
            "\n",
            "]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "As a Customer,I want to transfer money from my account to another bank account through ATM so that I may save my time.\n",
            "As a Customer,I want to transfer money from my account to another bank account through ATM \n",
            "[Customer, ,, want, transfer, money, account, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: transfer money\n",
            "want\n",
            "transfer --- using card\n",
            "transfer --- login\n",
            "transfer --- check the balance\n",
            "transfer --- deposit cash\n",
            "transfer --- deposit check\n",
            "transfer --- withdraw cash\n",
            "transfer --- transfer money\n",
            "transfer\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "\n",
            "\n",
            "\n",
            "]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hhPetaQ917n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "doc1=nlp(file)\n",
        "for i,sentenceVar in enumerate(doc1.sents):\n",
        "  print(i ,sentenceVar)\n",
        "  for ent in sentenceVar.ents:\n",
        "    #print(ent.text,\"  \",ent.pos_,\"   \",ent.tag_,\"    \",spacy.explain(ent.pos_))\n",
        "    print(ent.text,\"  \",ent.label)\n",
        "\n",
        "  print(\"_______________________________\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjOzAELtWvnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file=open('/content/drive/MyDrive/Colab Notebooks/university.txt')\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "\"\"\"\n",
        "for sentenceVar in doc1.sents:\n",
        "  print(sentenceVar)\n",
        "  print(\"new sentence\")\n",
        "\"\"\"\n",
        "\n",
        "from spacy.language import Language\n",
        "\n",
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"|\" and doc[i + 1].pos_=='DET':\n",
        "            doc[i + 1].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n",
        "doc = nlp(\"This is. A sentence This is. Another sentence.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@Language.component(\"set_custom_boundaries\")\n",
        "def set_custom_boundaries(doc):\n",
        "\n",
        "  for i, token in enumerate(doc[:-1]):\n",
        "    print(token)\n",
        "    if token.text == \".\" and doc[i + 1].pos_==\"ADP\":\n",
        "      doc[i + 1].is_sent_start = True\n",
        "    elif token.text != '.' and doc[i + 1].pos_==\"ADP\" :\n",
        "       doc[i + 1].is_sent_start = True\n",
        "    else:\n",
        "         # Explicitly set sentence start to False otherwise, to tell\n",
        "         # the parser to leave those tokens alone\n",
        "      doc[i + 1].is_sent_start = False\n",
        "\n",
        "\n",
        "    return doc\n",
        "\n",
        "\n",
        "nlp.add_pipe(\"set_custom_boundaries\",before=\"parser\")\n",
        "nlp.pipe_names\n",
        "  \n",
        "\n",
        "doc1=nlp(file.read())\n",
        "for sentenceVar in doc1.sents:\n",
        "  print(sentenceVar[0].pos_)\n",
        "  print(\"new sentence@@@@@@@@\")\n",
        "  print(sentenceVar)\n",
        "\n",
        "  \"\"\"\n",
        "i=0\n",
        "for x in doc1:\n",
        "  if x.is_sent_start:\n",
        "    i=i+1\n",
        "    print(x)\n",
        "print (i)\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "aIp3cq9FhK89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "215e57e7-429c-4496-9a14-4d21bfc5d70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-93f19fb69f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/university.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentenceVar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/university.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc = nlp(sentence)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "kK4KpMhZjPTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)\n"
      ],
      "metadata": {
        "id": "cJ6tzcaDjLk6"
      }
    }
  ]
}