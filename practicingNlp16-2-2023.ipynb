{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-King-00/Gp-AutomationOfBaTasks/blob/tony/practicingNlp16-2-2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")\n"
      ],
      "metadata": {
        "id": "8NOz5j97nrJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_to_classify = \"one day I will see the world\"\n",
        "candidate_labels = ['travel', 'cooking', 'dancing']\n",
        "classifier(sequence_to_classify, candidate_labels)\n",
        "#{'labels': ['travel', 'dancing', 'cooking'],\n",
        "# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n",
        "# 'sequence': 'one day I will see the world'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bFDn_cYoKFo",
        "outputId": "1f4b2a21-06af-4cbb-df70-f66cfb38f0a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'one day I will see the world',\n",
              " 'labels': ['travel', 'dancing', 'cooking'],\n",
              " 'scores': [0.9938651919364929, 0.0032737930305302143, 0.0028610294684767723]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install -U spacy\n",
        "from spacy.language import Language\n",
        "import spacy\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "gSixyAQyCOVF",
        "outputId": "93bd8b2c-5c2b-4556-af9b-ba7392c575ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install PyGithub\n",
        "from github import Github\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpBTVs_RdIyy",
        "outputId": "a0ea08a3-ed25-4f12-ea06-613f45d323aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-1.57-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.4.0\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.8/dist-packages (from PyGithub) (2.25.1)\n",
            "Collecting pyjwt>=2.4.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from pynacl>=1.4.0->PyGithub) (1.15.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated->PyGithub) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, deprecated, pynacl, PyGithub\n",
            "Successfully installed PyGithub-1.57 deprecated-1.2.13 pyjwt-2.6.0 pynacl-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-1]):\n",
        "    \n",
        "        if token.text == \".\"  :\n",
        "          doc[i + 1].is_sent_start = True\n",
        "        elif  token.text == \"As\":\n",
        "          doc[i].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxKoDEN8Twgk",
        "outputId": "2cccac76-326f-495b-87c1-d3212607c856"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.custom_sentencizer(doc)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/T-King-00/Gp-AutomationOfBaTasks/tony/atmUserStories.txt')\n",
        "file = response.text"
      ],
      "metadata": {
        "id": "GD14FYmzMr_T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SENTENCE=\"As a company, I manage a set of course registrations so that I can control which employees in my company have access to which courses.\"\n",
        "n=nlp(SENTENCE)\n",
        "for TOK in n:\n",
        "  print(TOK,\"  -> \",TOK.pos_,spacy.explain(TOK.pos_) )\n"
      ],
      "metadata": {
        "id": "SuLYPWWBYNYW",
        "outputId": "380913e4-3bd4-41ea-a048-1b1d5183db4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As   ->  ADP adposition\n",
            "a   ->  DET determiner\n",
            "company   ->  NOUN noun\n",
            ",   ->  PUNCT punctuation\n",
            "I   ->  PRON pronoun\n",
            "manage   ->  VERB verb\n",
            "a   ->  DET determiner\n",
            "set   ->  NOUN noun\n",
            "of   ->  ADP adposition\n",
            "course   ->  NOUN noun\n",
            "registrations   ->  NOUN noun\n",
            "so   ->  SCONJ subordinating conjunction\n",
            "that   ->  SCONJ subordinating conjunction\n",
            "I   ->  PRON pronoun\n",
            "can   ->  AUX auxiliary\n",
            "control   ->  VERB verb\n",
            "which   ->  PRON pronoun\n",
            "employees   ->  NOUN noun\n",
            "in   ->  ADP adposition\n",
            "my   ->  PRON pronoun\n",
            "company   ->  NOUN noun\n",
            "have   ->  VERB verb\n",
            "access   ->  NOUN noun\n",
            "to   ->  ADP adposition\n",
            "which   ->  PRON pronoun\n",
            "courses   ->  NOUN noun\n",
            ".   ->  PUNCT punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class UserStory():\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "   self.actors=[]\n",
        "   self.usecases=[]\n",
        " \n",
        "  def addActor(self,text):\n",
        "    if text not in self.actors :\n",
        "      self.actors.append(text)\n",
        "\n",
        "  def checkSimilarityB2UseCases(self,t1):\n",
        "    for x in self.usecases:\n",
        "      print(t1,\"---\",x)\n",
        "      v = re.search(t1,x)\n",
        "      if v :\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "  def addUseCase(self,text):\n",
        "\n",
        "    if text not in self.usecases and text!=\"want\":\n",
        "      if self.checkSimilarityB2UseCases(text):\n",
        "        self.usecases.append(text)\n",
        " \n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1leYVrsN7pDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###try 1\n",
        "doc = nlp(file)\n",
        "sentences=[]\n",
        "for sent in doc.sents:\n",
        "  sentences.append(sent.text)"
      ],
      "metadata": {
        "id": "T86Dq2m8Y6LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc.ents #used for named entites like websites \n",
        "\"\"\"\n",
        "This doc property is used for the named entities in the document. If the entity recognizer has been applied, this property will return a tuple of named entity span objects.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SeWuQNyJ_ZIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This part gets the actors in all user stories through looping through each sentence and getting noun chunks .**"
      ],
      "metadata": {
        "id": "BTI3rC0x_avK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USERSTORIES=UserStory()\n",
        "\n",
        "for sentenceObj in sentences:\n",
        "  objNlp=nlp(sentenceObj)\n",
        "\n",
        "  ### this part get compound nouns (actors))\n",
        "  for chunk in objNlp.noun_chunks:\n",
        "    USERSTORIES.addActor(chunk.text)\n",
        "    break\n",
        "\n",
        "USERSTORIES.actors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2RPiazqz1THZ",
        "outputId": "707ebe70-2c46-4512-9f16-ec31114d7b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a Customer']"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "  for ent in objNlp:\n",
        "    print(ent.text,\" -> \",ent.pos_,\"   \",ent.tag_,\"    \",spacy.explain(ent.pos_)) \n",
        "    if ent.pos_==\"NOUN\" :\n",
        "      USERSTORY.addActor(ent.text)\n",
        "      break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RASJ3frs92lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This part to get use cases of one actor .**"
      ],
      "metadata": {
        "id": "UIOFteGV_pSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "gdFt5lnPR0OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher=Matcher(nlp.vocab)\n",
        "\n",
        "for sentenceVar in sentences:\n",
        "  usecases=[]\n",
        "  print(sentenceVar)\n",
        "  v = sentenceVar.find(\"so that\")\n",
        "  sentenceVar=sentenceVar[0:v]\n",
        "  print(sentenceVar)  \n",
        "  x=nlp(sentenceVar)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "  tokens=[token for token in x if not token.is_stop]\n",
        "  print(tokens)\n",
        "\n",
        "  pattern0 = [[{\"LOWER\": \"want\"}, {\"LOWER\": \"to\" },{\"POS\":\"VERB\"},{\"POS\":\"DET\",\"OP\":\"*\"},{\"POS\":\"NOUN\"}]\n",
        "              ,[{\"LOWER\": \"make\"}, {\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern1=[{\"POS\":\"VERB\"},{\"POS\":\"NOUN\"}]\n",
        "  pattern2=[[{\"POS\":\"VERB\"},{\"POS\":\"DET\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"VERB\"},{\"LOWER\":\"the\"},{\"POS\":\"NOUN\"}]]\n",
        "\n",
        "  pattern3=[{\"POS\":\"NOUN\"},{\"POS\":\"NOUN\"}]\n",
        "  matcher.add(\"verbPhrase\", [pattern1])\n",
        "  matcher.add(\"verbPhrase2\", pattern2)\n",
        "\n",
        "\n",
        "  matches = matcher(x)\n",
        "  for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
        "      span = x[start:end]  # The matched span\n",
        "      print(match_id, string_id, start, end,\"text:\", span.text)\n",
        "  \n",
        "      USERSTORIES.addUseCase(span.text)\n",
        "\n",
        "  \n",
        "  for token in x:\n",
        "    if token.is_stop !=True:\n",
        "      \n",
        "      if token.pos_== \"VERB\" :\n",
        "        USERSTORIES.addUseCase(token.text)\n",
        "        print(token.text)\n",
        "  print(\"use cases are : \" ,USERSTORIES.usecases)\n",
        "\n",
        "  print(\"end of loopp#############################################\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhkUrl0O_mji",
        "outputId": "a50f9c99-fb9d-44d3-93db-bd5ec44b1f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Customer ,I want to login to my account using card and PIN code so that I can perform the transactions.\n",
            "As a Customer ,I want to login to my account using card and PIN code \n",
            "[Customer, ,, want, login, account, card, PIN, code]\n",
            "13289390234190212481 verbPhrase 11 13 text: using card\n",
            "want\n",
            "login --- using card\n",
            "login\n",
            "use cases are :  ['using card', 'login']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to  check the balance of my bank account so that I can perform transactions.\n",
            "As a Customer ,I want to  check the balance of my bank account \n",
            "[Customer, ,, want,  , check, balance, bank, account]\n",
            "7260299203751380718 verbPhrase2 8 11 text: check the balance\n",
            "check the balance --- using card\n",
            "check the balance --- login\n",
            "want\n",
            "check --- using card\n",
            "check --- login\n",
            "check --- check the balance\n",
            "check\n",
            "use cases are :  ['using card', 'login', 'check the balance']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to  deposit cash in my bank account through ATM so that I may save my time and perform transactions later.\n",
            "As a Customer ,I want to  deposit cash in my bank account through ATM \n",
            "[Customer, ,, want,  , deposit, cash, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 8 10 text: deposit cash\n",
            "deposit cash --- using card\n",
            "deposit cash --- login\n",
            "deposit cash --- check the balance\n",
            "want\n",
            "deposit --- using card\n",
            "deposit --- login\n",
            "deposit --- check the balance\n",
            "deposit --- deposit cash\n",
            "deposit\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash']\n",
            "end of loopp#############################################\n",
            "As a Customer ,I want to deposit check in my bank account through ATM so that I may save my time and perform transactions later.\n",
            "As a Customer ,I want to deposit check in my bank account through ATM \n",
            "[Customer, ,, want, deposit, check, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: deposit check\n",
            "deposit check --- using card\n",
            "deposit check --- login\n",
            "deposit check --- check the balance\n",
            "deposit check --- deposit cash\n",
            "want\n",
            "deposit --- using card\n",
            "deposit --- login\n",
            "deposit --- check the balance\n",
            "deposit --- deposit cash\n",
            "deposit\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check']\n",
            "end of loopp#############################################\n",
            "As a Customer I want to withdraw cash from my bank account through ATM so that I may save my time.\n",
            "As a Customer I want to withdraw cash from my bank account through ATM \n",
            "[Customer, want, withdraw, cash, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 6 8 text: withdraw cash\n",
            "withdraw cash --- using card\n",
            "withdraw cash --- login\n",
            "withdraw cash --- check the balance\n",
            "withdraw cash --- deposit cash\n",
            "withdraw cash --- deposit check\n",
            "want\n",
            "withdraw --- using card\n",
            "withdraw --- login\n",
            "withdraw --- check the balance\n",
            "withdraw --- deposit cash\n",
            "withdraw --- deposit check\n",
            "withdraw --- withdraw cash\n",
            "withdraw\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash']\n",
            "end of loopp#############################################\n",
            "\r\n",
            "\n",
            "\r\n",
            "[\r]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash']\n",
            "end of loopp#############################################\n",
            "As a Customer, I want to transfer money from my account to another bank account through ATM so that I may save my time.\n",
            "As a Customer, I want to transfer money from my account to another bank account through ATM \n",
            "[Customer, ,, want, transfer, money, account, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: transfer money\n",
            "transfer money --- using card\n",
            "transfer money --- login\n",
            "transfer money --- check the balance\n",
            "transfer money --- deposit cash\n",
            "transfer money --- deposit check\n",
            "transfer money --- withdraw cash\n",
            "want\n",
            "transfer --- using card\n",
            "transfer --- login\n",
            "transfer --- check the balance\n",
            "transfer --- deposit cash\n",
            "transfer --- deposit check\n",
            "transfer --- withdraw cash\n",
            "transfer --- transfer money\n",
            "transfer\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "\n",
            "\n",
            "\n",
            "]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "As a Customer,I want to transfer money from my account to another bank account through ATM so that I may save my time.\n",
            "As a Customer,I want to transfer money from my account to another bank account through ATM \n",
            "[Customer, ,, want, transfer, money, account, bank, account, ATM]\n",
            "13289390234190212481 verbPhrase 7 9 text: transfer money\n",
            "want\n",
            "transfer --- using card\n",
            "transfer --- login\n",
            "transfer --- check the balance\n",
            "transfer --- deposit cash\n",
            "transfer --- deposit check\n",
            "transfer --- withdraw cash\n",
            "transfer --- transfer money\n",
            "transfer\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n",
            "\n",
            "\n",
            "\n",
            "]\n",
            "use cases are :  ['using card', 'login', 'check the balance', 'deposit cash', 'deposit check', 'withdraw cash', 'transfer money']\n",
            "end of loopp#############################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hhPetaQ917n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "doc1=nlp(file)\n",
        "for i,sentenceVar in enumerate(doc1.sents):\n",
        "  print(i ,sentenceVar)\n",
        "  for ent in sentenceVar.ents:\n",
        "    #print(ent.text,\"  \",ent.pos_,\"   \",ent.tag_,\"    \",spacy.explain(ent.pos_))\n",
        "    print(ent.text,\"  \",ent.label)\n",
        "\n",
        "  print(\"_______________________________\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjOzAELtWvnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file=open('/content/drive/MyDrive/Colab Notebooks/university.txt')\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "\"\"\"\n",
        "for sentenceVar in doc1.sents:\n",
        "  print(sentenceVar)\n",
        "  print(\"new sentence\")\n",
        "\"\"\"\n",
        "\n",
        "from spacy.language import Language\n",
        "\n",
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"|\" and doc[i + 1].pos_=='DET':\n",
        "            doc[i + 1].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n",
        "doc = nlp(\"This is. A sentence This is. Another sentence.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@Language.component(\"set_custom_boundaries\")\n",
        "def set_custom_boundaries(doc):\n",
        "\n",
        "  for i, token in enumerate(doc[:-1]):\n",
        "    print(token)\n",
        "    if token.text == \".\" and doc[i + 1].pos_==\"ADP\":\n",
        "      doc[i + 1].is_sent_start = True\n",
        "    elif token.text != '.' and doc[i + 1].pos_==\"ADP\" :\n",
        "       doc[i + 1].is_sent_start = True\n",
        "    else:\n",
        "         # Explicitly set sentence start to False otherwise, to tell\n",
        "         # the parser to leave those tokens alone\n",
        "      doc[i + 1].is_sent_start = False\n",
        "\n",
        "\n",
        "    return doc\n",
        "\n",
        "\n",
        "nlp.add_pipe(\"set_custom_boundaries\",before=\"parser\")\n",
        "nlp.pipe_names\n",
        "  \n",
        "\n",
        "doc1=nlp(file.read())\n",
        "for sentenceVar in doc1.sents:\n",
        "  print(sentenceVar[0].pos_)\n",
        "  print(\"new sentence@@@@@@@@\")\n",
        "  print(sentenceVar)\n",
        "\n",
        "  \"\"\"\n",
        "i=0\n",
        "for x in doc1:\n",
        "  if x.is_sent_start:\n",
        "    i=i+1\n",
        "    print(x)\n",
        "print (i)\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "aIp3cq9FhK89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "215e57e7-429c-4496-9a14-4d21bfc5d70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-93f19fb69f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/university.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentenceVar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/university.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc = nlp(sentence)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "kK4KpMhZjPTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)\n"
      ],
      "metadata": {
        "id": "cJ6tzcaDjLk6"
      }
    }
  ]
}